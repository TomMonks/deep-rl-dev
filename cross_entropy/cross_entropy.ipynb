{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Method for RL\n",
    "\n",
    "The cross-entropy method is model free, policy based and on-policy.  This means that\n",
    "\n",
    "* It does build any model of the environment.  It just maps state to action.\n",
    "* It approximates the policy (using a neural network)\n",
    "* It gets its data from the environment sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#gym \n",
    "import gym\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import time\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "TARGET = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used is neural network with one fully connected layer that maps an input vector size `input_size` to `hidden_size`, a ReLU activation, and a output layer of size `n_actions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_actions):\n",
    "        # Inherit parent (nn.module) methods using super init\n",
    "        super(SingleLayerNetwork, self).__init__()\n",
    "  \n",
    "        #The first layer maps input features to a second layer\n",
    "        #with user specified number of neurons.\n",
    "        self.fc1 = nn.Linear(in_features=input_size, \n",
    "                             out_features=hidden_size, \n",
    "                             bias=True)\n",
    "        \n",
    "        #The output layer has a user specified n_actions\n",
    "        self.out = nn.Linear(in_features=hidden_size,\n",
    "                             out_features=n_actions,\n",
    "                             bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass data through net. \n",
    "        x = F.relu(self.fc1(x))\n",
    "        y_pred = self.out(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env, model):    \n",
    "    '''\n",
    "    Generates the batches and episodes\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    env: gym environment\n",
    "    \n",
    "    model: nn.module\n",
    "        neural network model\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    #softmax layer: so outputs sum to 1.0\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    \n",
    "    #tracking\n",
    "    batch = [] # list of episode instances\n",
    "    episode_reward = 0.0 # total reward for current episode \n",
    "    episode_steps = [] # episode step objects\n",
    "    \n",
    "    #tracking states encountered and actions taken...\n",
    "    obs_history = []\n",
    "    action_history = []\n",
    "    \n",
    "    #reset gym environment and get initial observation\n",
    "    obs = env.reset() \n",
    "    \n",
    "    #episode has maximum 200 steps...\n",
    "    for i in range(200):\n",
    "        \n",
    "        #track state/observation\n",
    "        obs_history.append(obs)\n",
    "        \n",
    "        #pytorch requires state as tensor\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        \n",
    "        #predict actions probabilities (logits)\n",
    "        action_probs_v = model(obs_v)\n",
    "        \n",
    "        #normalise using softmax\n",
    "        action_probs_v = sm(action_probs_v)\n",
    "        \n",
    "        #convert to numpy\n",
    "        action_probs = action_probs_v.data.numpy()[0]\n",
    " \n",
    "        #take a random action\n",
    "        action = np.random.choice(len(action_probs), p=action_probs)\n",
    "        next_obs, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        #track cumulative reward and action taken\n",
    "        episode_reward += reward\n",
    "        action_history.append(action)\n",
    "        \n",
    "        #if end of episode\n",
    "        if terminal:\n",
    "            break\n",
    "            \n",
    "        obs = next_obs\n",
    "    \n",
    "    #return episode as a dict\n",
    "    return {'reward':episode_reward,\n",
    "            'states':obs_history,\n",
    "            'actions':action_history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyAgent(object):\n",
    "    '''\n",
    "    Deep Reinforcement Agent based on Cross Entropy Method.\n",
    "    \n",
    "    The cross-entropy method is model free, policy based and on-policy.      \n",
    "    '''\n",
    "    def __init__(self, environment, target, batch_size, percentile=70,\n",
    "                 neural_network=None, lr=0.01, show_every=2):\n",
    "        '''\n",
    "        Params:\n",
    "        -----\n",
    "        '''\n",
    "        self.env = environment\n",
    "        self.target = target\n",
    "        self.batch_size = batch_size\n",
    "        self.percentile = percentile\n",
    "        self.show = show_every\n",
    "        \n",
    "        #default model\n",
    "        if neural_network is None:\n",
    "            #observation size and no. actions\n",
    "            obs_size = self.env.observation_space.shape[0]\n",
    "            n_actions = self.env.action_space.n\n",
    "            self.model = SingleLayerNetwork(obs_size, HIDDEN_SIZE, n_actions)\n",
    "        else:\n",
    "            self.model = neural_network\n",
    "            \n",
    "        self.objective = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(params=self.model.parameters(), \n",
    "                                          lr=lr)\n",
    "    \n",
    "    def solve(self):\n",
    "\n",
    "        batch_count = 0\n",
    "    \n",
    "        #learn until target is reached.\n",
    "        while True:\n",
    "\n",
    "            # Play episodes \n",
    "            batch_count += 1\n",
    "            batch = []\n",
    "            for episode in range(self.batch_size):\n",
    "                episode_results = self.play_episode()\n",
    "                batch.append(episode_results)\n",
    "\n",
    "            #filter elite episodes\n",
    "            obs_v, acts_v, reward_b, reward_m = self.elite_episodes(batch)\n",
    "\n",
    "            #train on elite episodes\n",
    "            self.optimizer.zero_grad()\n",
    "            action_scores_v = self.model(obs_v)\n",
    "            loss = self.objective(action_scores_v, acts_v)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            \n",
    "            if batch_count%self.show == 0:\n",
    "                print(f'{batch_count}: loss={loss.item():.3f},' \\\n",
    "                      + f'\\t\\tavg reward={reward_m:.1f},\\treward_bound={reward_b:.1f}')\n",
    "\n",
    "                \n",
    "            if reward_m >= self.target:\n",
    "                print(f'solved at iter {batch_count}. mean_reward={reward_m}')\n",
    "                break\n",
    "        \n",
    "    \n",
    "    def play_episode(self):    \n",
    "        '''\n",
    "        Generates the batches and episodes\n",
    "\n",
    "        Parameters:\n",
    "        ------\n",
    "        env: gym environment\n",
    "\n",
    "        model: nn.module\n",
    "            neural network model\n",
    "\n",
    "        '''\n",
    "\n",
    "        #softmax layer: so outputs sum to 1.0\n",
    "        sm = nn.Softmax(dim=1)\n",
    "\n",
    "        #tracking\n",
    "        batch = [] # list of episode instances\n",
    "        episode_reward = 0.0 # total reward for current episode \n",
    "        episode_steps = [] # episode step objects\n",
    "\n",
    "        #tracking states encountered and actions taken...\n",
    "        obs_history = []\n",
    "        action_history = []\n",
    "\n",
    "        #reset gym environment and get initial observation\n",
    "        obs = self.env.reset() \n",
    "\n",
    "        #episode has maximum 200 steps...\n",
    "        for i in range(200):\n",
    "\n",
    "            #track state/observation\n",
    "            obs_history.append(obs)\n",
    "\n",
    "            #pytorch requires state as tensor\n",
    "            obs_v = torch.FloatTensor([obs])\n",
    "\n",
    "            #predict actions probabilities (logits)\n",
    "            action_probs_v = self.model(obs_v)\n",
    "\n",
    "            #normalise using softmax\n",
    "            action_probs_v = sm(action_probs_v)\n",
    "\n",
    "            #convert to numpy\n",
    "            action_probs = action_probs_v.data.numpy()[0]\n",
    "\n",
    "            #take a random action\n",
    "            action = np.random.choice(len(action_probs), p=action_probs)\n",
    "            next_obs, reward, terminal, _ = self.env.step(action)\n",
    "\n",
    "            #track cumulative reward and action taken\n",
    "            episode_reward += reward\n",
    "            action_history.append(action)\n",
    "\n",
    "            #if end of episode\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        #return episode as a dict\n",
    "        return {'reward':episode_reward,\n",
    "                'states':obs_history,\n",
    "                'actions':action_history}\n",
    "    \n",
    "    \n",
    "    def elite_episodes(self, batch):\n",
    "\n",
    "        rewards = np.array([b['reward'] for b in batch])\n",
    "        reward_bound = np.percentile(rewards, self.percentile)\n",
    "        reward_mean = float(np.mean(rewards))\n",
    "\n",
    "        train_obs = []\n",
    "        train_act = []\n",
    "\n",
    "        for episode in np.array(batch)[rewards >= reward_bound]:\n",
    "            train_obs.extend(episode['states'])\n",
    "            train_act.extend(episode['actions'])\n",
    "\n",
    "        train_obs = torch.FloatTensor(train_obs)\n",
    "        train_act = torch.LongTensor(train_act)\n",
    "\n",
    "        return train_obs, train_act, reward_bound, reward_mean      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: loss=0.684,\t\tavg reward=19.9,\treward_bound=23.0\n",
      "4: loss=0.671,\t\tavg reward=18.7,\treward_bound=21.0\n",
      "6: loss=0.663,\t\tavg reward=24.8,\treward_bound=27.0\n",
      "8: loss=0.668,\t\tavg reward=42.8,\treward_bound=49.5\n",
      "10: loss=0.640,\t\tavg reward=71.9,\treward_bound=86.5\n",
      "12: loss=0.627,\t\tavg reward=50.6,\treward_bound=60.0\n",
      "14: loss=0.616,\t\tavg reward=76.9,\treward_bound=89.5\n",
      "16: loss=0.592,\t\tavg reward=93.0,\treward_bound=103.5\n",
      "18: loss=0.597,\t\tavg reward=90.4,\treward_bound=100.5\n",
      "20: loss=0.561,\t\tavg reward=91.2,\treward_bound=102.5\n",
      "22: loss=0.561,\t\tavg reward=108.9,\treward_bound=133.5\n",
      "24: loss=0.552,\t\tavg reward=119.1,\treward_bound=135.5\n",
      "26: loss=0.561,\t\tavg reward=153.6,\treward_bound=161.5\n",
      "28: loss=0.541,\t\tavg reward=145.9,\treward_bound=175.5\n"
     ]
    }
   ],
   "source": [
    "#gym environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "#agent\n",
    "agent = CrossEntropyAgent(environment=env, \n",
    "                          target=TARGET, \n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          percentile=PERCENTILE)\n",
    "\n",
    "agent.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('pytorchenv': conda)",
   "language": "python",
   "name": "python37364bitpytorchenvconda884da03c229e4c1b9901e8e9a2d74915"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
